<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <!-- Begin Jekyll SEO tag v2.7.1 -->
  <title>VITS-Based Singing Voice Conversion Leveraging Whisper and multi-scale F0 Modeling</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="TODO: title">
  <meta property="og:locale" content="en_US">
  <meta name="twitter:card" content="summary">
  <!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->


  </section>

  <section class="main-content">
    <h1 id="">
      <center>VITS-Based Singing Voice Conversion Leveraging Whisper and multi-scale F0 Modeling</center>
    </h1>

    <h3 id="">
      <center>Ziqian Ning<sup>1</sup>, Yuepeng Jiang<sup>1</sup>, Zhichao Wang<sup>1</sup>, Bin Zhang<sup>2</sup>, Lei Xie<sup>1</sup></center>
      <center><sup>1</sup>Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi'an, China</center>
      <center><sup>2</sup>TME Lyra Lab, Tencent Inc., Shenzhen, China</center>
    </h3>
    <center>Accepted by ASRU 2023</center>


    <br><br>
    <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
    <p>This paper presents the T23 team's system for the Singing Voice Conversion Challenge 2023. Our singing conversion model is built on VITS, integrating a prior encoder, a posterior encoder, a decoder, and a parallel bank of transposed convolutions (PBTC) module. Leveraging Whisper ASR model, we extract bottleneck features (BNF) as input for the prior encoder. We use pitch perturbation to remove speaker timbre before BNF extraction, preventing leakage of source speaker timbre. The PBTC module extracts multi-scale F0, enhancing pitch variations in singing. A three-stage training strategy adapts the base model to the target speaker with limited data. Official challenge results show our system ranks 1st and 2nd in Task 1 and 2, exhibiting superior naturalness. Ablation study confirms the effectiveness of our system design.</p>

    <table frame=void rules=none>
      <tr>
        <center><img src='raw/fig/vits-traininfer-1.png' width="60%"></center>
      </tr>
      <tr>
      </tr>
    </table>
    <br><br>


    <h2>2. Demos -- Singing Voice Conversion<a name="Comparison"></a></h2>
    <p>
      The challenge organizers provide two singers (IDF1 and IDM1) in any-to-one, in-domain singing voice conversion (task 1); two speakers (CDF1, CDM1) in any-to-one, cross-domain singing voice conversion (task2).In task 2, only speech data is provided.
    </p>
    <ul>
      <!-- <li>Bottomline: Replace all convolution layers in the backbone model [1] with the causal version to create a naïve streaming implementation.</li>
      <li>IBF-VC: Reinplement [2] using the backbone model.</li>
      <li>Topline: Use the unmodified Backbone model which leverage full-context, but input BNF is extracted from a streaming ASR encoder.</li>
      <li>DualVC-nonstreaming: Non-streaming mode of our proposed DualVC.</li>
      <li>DualVC-streaming: Streaming mode of our proposed DualVC.</li> -->

      <!-- <li>SF1 and SM1: source speakers</li>
      <li>IDF1, IDM1, CDF1 and CDM1: target speakers</li> -->
    </ul>

    <table>
      <tbody id="tbody">
      </tbody>
    </table>
    </br>
    <cite>
      <a href="https://arxiv.org/abs/2306.14422">Wen-Chin Huang, Lester Phillip Violeta, Songxiang Liu, Jiatong Shi, Yusuke Yasuda, and Tomoki Toda, “The singing voice conversion challenge 2023,” CoRR, vol. abs/2306.14422, 2023.</a>
    </cite>
    <br>
    <cite>
      <a href="https://arxiv.org/abs/2303.12197">Tejas Jayashankar, Jilong Wu, Leda Sari, David Kant, Vimal Manohar, and Qing He, “Self-supervised representations for singing voice conversion,” in Proc. ICASSP. IEEE, 2023, pp. 1–5.</a>
    </cite>
    <br>
    <cite>
      <a href="https://ieeexplore.ieee.org/document/6853678">Matthias Mauch and Simon Dixon, “PYIN: A fundamental frequency estimator using probabilistic threshold distributions,” in Proc. ICASSP. 2014, pp. 659–663, IEEE.</a>
    </cite>
  </section>
</body>

</html>

<script type="" text/javascript>
  window.onload = function () {
    let scenes = ["SF1", "SM1"]
    let models = ["IDF1", "IDM1", "CDF1", "CDM1"]
    let all_samples = ["30006", "30009", "30015", "30024"]
    let sample_data = `
        <tr>
          <td style="text-align: center; width: 150px;" rowspan=3><strong>Source Speaker<strong></td>
          <td style="text-align: center; width: 150px;" rowspan=3><strong>Source Singing<strong></td>
          <td style="text-align: center; width: 150px;" colspan=8><strong>Target Speaker<strong></td>
        </tr>
        <tr>
        `
    for (const id in models) {
      model = models[id]
      sample_data += '<td style="text-align: center; width: 150px;" rowspan=1 colspan=2><strong>' + model + '<strong></td>'
    }
    sample_data += `</tr><tr>`
    for (const id in models) {
      sample_data += '<td style="text-align: center; width: 150px;" rowspan=1 colspan=1>reference</td>'
      sample_data += '<td style="text-align: center; width: 150px;" rowspan=1 colspan=1>converted singing</td>'
    }
    
    
    console.log(sample_data)
    
    for (let x in scenes) {
      let scene = scenes[x]
      let scene_data = ""
      scene_data += '<tr>'
      scene_data += '<td style="text-align: center; width: 150px;" rowspan=' + 4 + '><strong>' + scene + '<strong></td>'
      let example_place = 0
      for (let z in all_samples) {
        if (z != 0) {
          scene_data += '<tr>'
        }
        let sample = all_samples[z]
        scene_data += '<td style="text-align: center"><audio style="width: 150px;" controls="" src="' + './raw/samples/' + scene + '/source/' + sample + '_' + scene + '.wav' + '"></audio></td>'
        for (let w in models) {
          let model = models[w]
          if (example_place < 4) {
            scene_data += '<td style="text-align: center" rowspan=4><audio style="width: 150px;" controls="" src="' + './raw/samples/target/'  + model + '.wav' + '"></audio></td>'
            example_place += 1
          }
          scene_data += '<td style="text-align: center"><audio style="width: 150px;" controls="" src="' + './raw/samples/' + scene + '/' + model + '/' + sample + '.wav' + '"></audio></td>'
        }
        scene_data += '</tr>'
      }
      sample_data += scene_data
    }
    document.getElementById('tbody').innerHTML = sample_data
  }
</script>